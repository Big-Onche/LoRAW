{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pathlib import Path\n",
    "import IPython.display as ipd\n",
    "\n",
    "from util.platform import get_torch_device_type\n",
    "from diffusion_library.sampler import SamplerType\n",
    "from diffusion_library.scheduler import SchedulerType\n",
    "\n",
    "from train_uncond_lora import DiffusionUncondLora, ExceptionCallback, DemoCallback\n",
    "from dataset.dataset import SampleDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "args.name = 'loraw_dev'\n",
    "args.checkpoint_every = 400\n",
    "args.demo_every = 25\n",
    "args.num_demos = 1\n",
    "args.demo_samples = 65536\n",
    "args.demo_steps = 50\n",
    "args.accum_batches = 4\n",
    "\n",
    "args.sample_size = 32768\n",
    "args.sample_rate = 16000\n",
    "args.latent_dim = 0\n",
    "args.seed = 0\n",
    "args.batch_size = 1\n",
    "args.max_epochs = 10\n",
    "args.lora_rank = 4\n",
    "\n",
    "args.ema_decay = 0.95\n",
    "args.random_crop = False\n",
    "args.num_gpus = 1\n",
    "args.cache_training_data = False\n",
    "\n",
    "device_type_accelerator = get_torch_device_type()\n",
    "device_accelerator = torch.device(device_type_accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = pl.loggers.WandbLogger(project=args.name)\n",
    "torch.manual_seed(seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generate(\n",
    "        model,\n",
    "        batch_size=1,\n",
    "        steps=50,\n",
    "        scheduler=SchedulerType.V_CRASH,\n",
    "        scheduler_args={\n",
    "            'sigma_min': 0.1,\n",
    "            'sigma_max': 50.0,\n",
    "            'rho': 1.0\n",
    "        },\n",
    "        sampler=SamplerType.V_IPLMS,\n",
    "        sampler_args={'use_tqdm': True},\n",
    "        callback=None\n",
    "):\n",
    "    generator = torch.Generator(device_accelerator)\n",
    "    generator.manual_seed(args.seed)\n",
    "    \n",
    "    step_list = scheduler.get_step_list(steps, device_accelerator.type, **scheduler_args)\n",
    "    \n",
    "    if SamplerType.is_v_sampler(sampler):\n",
    "        x_T = torch.randn([batch_size, 2, args.sample_size], generator=generator, device=device_accelerator)\n",
    "\n",
    "    return sampler.sample(\n",
    "        model,\n",
    "        x_T,\n",
    "        step_list,\n",
    "        callback,\n",
    "        **sampler_args\n",
    "    ).float()\n",
    "\n",
    "def preview_batch(generated):\n",
    "    for ix, gen_sample in enumerate(generated):\n",
    "        print(f'sample #{ix + 1}')\n",
    "        display(ipd.Audio(gen_sample.cpu(), rate=args.sample_rate))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model\n",
    "model_name = 'maestro_16000_65536'\n",
    "model_artifact = wandb_logger.use_artifact(f'{model_name}:v0')\n",
    "checkpoint_path = Path(model_artifact.download()) / f'{model_name}.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = 'models/maestro_16000_65536.ckpt'\n",
    "\n",
    "model = DiffusionUncondLora.load_from_checkpoint(checkpoint_path, map_location=device_accelerator, global_args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate baseline samples\n",
    "if debug:\n",
    "    batch_baseline = test_generate(model.diffusion, batch_size=2)\n",
    "    preview_batch(batch_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inject LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inject_new_lora(lora_dim=args.lora_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with lora (should theoretically sound the same with blank lora)\n",
    "if debug:\n",
    "    batch_empty = test_generate(model.diffusion, batch_size=2)\n",
    "    preview_batch(batch_empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "training_dir = 'input/ivq_16000_65536'\n",
    "train_set = SampleDataset([training_dir], args)\n",
    "train_dl = data.DataLoader(\n",
    "    train_set,\n",
    "    args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exc_callback = ExceptionCallback()\n",
    "ckpt_callback = pl.callbacks.ModelCheckpoint(every_n_train_steps=args.checkpoint_every, save_top_k=-1, dirpath='output')\n",
    "demo_callback = DemoCallback(args)\n",
    "\n",
    "wandb_logger.watch(model)\n",
    "wandb_logger.config = args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_trainer = pl.Trainer(\n",
    "    devices=args.num_gpus,\n",
    "    accelerator=\"gpu\",\n",
    "    num_nodes = args.num_nodes,\n",
    "    strategy='ddp',\n",
    "    precision=16,\n",
    "    accumulate_grad_batches=args.accum_batches, \n",
    "    callbacks=[ckpt_callback, demo_callback, exc_callback],\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=1,\n",
    "    max_epochs=args.max_epochs,\n",
    "\n",
    ") if args.num_gpus > 1 else pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=16,\n",
    "    accumulate_grad_batches=args.accum_batches,\n",
    "    callbacks=[ckpt_callback, demo_callback, exc_callback],\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=1,\n",
    "    max_epochs=args.max_epochs,\n",
    ")\n",
    "\n",
    "diffusion_trainer.fit(model, train_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
