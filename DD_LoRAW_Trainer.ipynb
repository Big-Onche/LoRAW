{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMinJChT7SAY9Zdw7pRKv4C"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfCG0rXlRsAO",
        "outputId": "98aca03b-7e33-41fc-8314-cf520aa89042"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get & Import Requirements\n",
        "!git clone https://github.com/Bikecicle/LoRAW\n",
        "%cd LoRAW\n",
        "!pip install git+https://github.com/diontimmer/sample-diffusion-lib\n",
        "!pip install pytorch_lightning\n",
        "!pip install prefigure\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from pathlib import Path\n",
        "import IPython.display as ipd\n",
        "\n",
        "from sample_diffusion.util.platform import get_torch_device_type\n",
        "from sample_diffusion.diffusion_library.sampler import SamplerType\n",
        "from sample_diffusion.diffusion_library.scheduler import SchedulerType\n",
        "\n",
        "from train_uncond_lora import DiffusionUncondLora, ExceptionCallback, DemoCallback\n",
        "from dataset.dataset import SampleDataset\n",
        "\n",
        "class Object(object):\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "GEWQKp7lSTwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Settings\n",
        "run_name = \"Rank 16\" #@param {type:\"string\"}\n",
        "project_name = 'loraw_test' #@param {type:\"string\"}\n",
        "checkpoint_every = 100 #@param {type:\"integer\"}\n",
        "demo_every = 25 #@param {type:\"integer\"}\n",
        "num_demos = 1 #@param {type:\"integer\"}\n",
        "demo_samples = 65536 #@param {type:\"integer\"}\n",
        "demo_steps = 50 #@param {type:\"integer\"}\n",
        "accum_batches = 1 #@param {type:\"integer\"}\n",
        "training_dir = '/content/drive/MyDrive/AI/datasets/dub_neuro' #@param {type:\"string\"}\n",
        "base_model_path = '/content/drive/MyDrive/AI/models/DanceDiffusion/dd/base_models/jmann-large-580k.ckpt' #@param {type:\"string\"}\n",
        "\n",
        "sample_size = 65536 #@param {type:\"integer\"}\n",
        "sample_rate = 48000 #@param {type:\"integer\"}\n",
        "latent_dim = 0 #@param {type:\"integer\"}\n",
        "seed = 0 #@param {type:\"integer\"}\n",
        "batch_size = 4 #@param {type:\"integer\"}\n",
        "max_epochs = 1000000 #@param {type:\"integer\"}\n",
        "lora_rank = 16 #@param {type:\"integer\"}\n",
        "\n",
        "ema_decay = 0.95 #@param {type:\"number\"}\n",
        "random_crop = False #@param {type:\"boolean\"}\n",
        "num_gpus = 1 #@param {type:\"integer\"}\n",
        "cache_training_data = False #@{type:\"boolean\"}\n",
        "\n",
        "args = Object()\n",
        "\n",
        "args.run_name = run_name\n",
        "args.project_name = project_name\n",
        "args.checkpoint_every = checkpoint_every\n",
        "args.demo_every = demo_every\n",
        "args.num_demos = num_demos\n",
        "args.demo_samples = demo_samples\n",
        "args.demo_steps = demo_steps\n",
        "args.accum_batches = accum_batches\n",
        "args.training_dir = training_dir\n",
        "args.base_model_path = base_model_path\n",
        "\n",
        "args.sample_size = sample_size\n",
        "args.sample_rate = sample_rate\n",
        "args.latent_dim = latent_dim\n",
        "args.seed = seed\n",
        "args.batch_size = batch_size\n",
        "args.max_epochs = max_epochs\n",
        "args.lora_rank = lora_rank\n",
        "args.ema_decay = ema_decay\n",
        "args.random_crop = random_crop\n",
        "args.num_gpus = num_gpus\n",
        "args.cache_training_data = cache_training_data\n",
        "\n",
        "device_type_accelerator = get_torch_device_type()\n",
        "device_accelerator = torch.device(device_type_accelerator)\n",
        "\n",
        "\n",
        "\n",
        "wandb_logger = pl.loggers.WandbLogger(project=args.project_name, name=args.run_name)\n",
        "torch.manual_seed(seed=args.seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "cellView": "form",
        "id": "aPwjlTfkWh72",
        "outputId": "5b603f2d-c56c-4495-9cc8-f11a5ea89597"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>./wandb/run-20230711_005046-tn81pdvx</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dreamwalker/loraw_test/runs/tn81pdvx' target=\"_blank\">Rank 16</a></strong> to <a href='https://wandb.ai/dreamwalker/loraw_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dreamwalker/loraw_test' target=\"_blank\">https://wandb.ai/dreamwalker/loraw_test</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dreamwalker/loraw_test/runs/tn81pdvx' target=\"_blank\">https://wandb.ai/dreamwalker/loraw_test/runs/tn81pdvx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f7b701f2af0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Model & Inject\n",
        "model = DiffusionUncondLora.load_from_checkpoint(args.base_model_path, map_location=device_accelerator, global_args=args, strict=False)\n",
        "model.inject_new_lora(lora_dim=args.lora_rank)\n",
        "model.to(device_accelerator)\n"
      ],
      "metadata": {
        "id": "Up_ME6wlZVcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start Training\n",
        "\n",
        "# Load dataset\n",
        "train_set = SampleDataset([args.training_dir], args)\n",
        "train_dl = data.DataLoader(\n",
        "    train_set,\n",
        "    args.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "\n",
        "from typing import Any\n",
        "\n",
        "class HijackedModelCheckpoint(pl.callbacks.ModelCheckpoint):\n",
        "    def _save_checkpoint(self, trainer: \"pl.Trainer\", filepath: str) -> None:\n",
        "        trainer.model.lora.save_weights(filepath, device_accelerator)\n",
        "\n",
        "        self._last_global_step_saved = trainer.global_step\n",
        "\n",
        "        # notify loggers\n",
        "        if trainer.is_global_zero:\n",
        "            for logger in trainer.loggers:\n",
        "                logger.after_save_checkpoint(proxy(self))\n",
        "\n",
        "exc_callback = ExceptionCallback()\n",
        "ckpt_callback = HijackedModelCheckpoint(every_n_train_steps=args.checkpoint_every, save_top_k=-1, dirpath='output')\n",
        "demo_callback = DemoCallback(args)\n",
        "\n",
        "wandb_logger.watch(model)\n",
        "wandb_logger.config = args\n",
        "\n",
        "diffusion_trainer = pl.Trainer(\n",
        "    devices=args.num_gpus,\n",
        "    accelerator=\"gpu\",\n",
        "    num_nodes = args.num_nodes,\n",
        "    strategy='ddp',\n",
        "    precision=16,\n",
        "    accumulate_grad_batches=args.accum_batches,\n",
        "    callbacks=[ckpt_callback, demo_callback, exc_callback],\n",
        "    logger=wandb_logger,\n",
        "    log_every_n_steps=1,\n",
        "    max_epochs=args.max_epochs,\n",
        "\n",
        ") if args.num_gpus > 1 else pl.Trainer(\n",
        "    devices=1,\n",
        "    accelerator=\"gpu\",\n",
        "    precision=16,\n",
        "    accumulate_grad_batches=args.accum_batches,\n",
        "    callbacks=[ckpt_callback, demo_callback, exc_callback],\n",
        "    logger=wandb_logger,\n",
        "    log_every_n_steps=1,\n",
        "    max_epochs=args.max_epochs,\n",
        ")\n",
        "\n",
        "diffusion_trainer.fit(model, train_dl)\n",
        "import wandb\n",
        "wandb.finish()\n",
        "\n"
      ],
      "metadata": {
        "id": "c8WFUONKYCDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}