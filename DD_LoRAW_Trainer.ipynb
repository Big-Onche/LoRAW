{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNrUIaRw3zI+GwXnM5zMDa7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfCG0rXlRsAO"
      },
      "outputs": [],
      "source": [
        "#@title Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get & Import Requirements\n",
        "!git clone https://github.com/Bikecicle/LoRAW\n",
        "%cd LoRAW\n",
        "!pip install git+https://github.com/diontimmer/sample-diffusion-lib\n",
        "!pip install pytorch_lightning\n",
        "!pip install prefigure\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from pathlib import Path\n",
        "import IPython.display as ipd\n",
        "\n",
        "from sample_diffusion.util.platform import get_torch_device_type\n",
        "from sample_diffusion.diffusion_library.sampler import SamplerType\n",
        "from sample_diffusion.diffusion_library.scheduler import SchedulerType\n",
        "\n",
        "from train_uncond_lora import DiffusionUncondLora, ExceptionCallback, DemoCallback\n",
        "from dataset.dataset import SampleDataset\n",
        "\n",
        "class Object(object):\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "GEWQKp7lSTwf",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Settings\n",
        "run_name = \"Rank 16\" #@param {type:\"string\"}\n",
        "project_name = 'loraw_test' #@param {type:\"string\"}\n",
        "checkpoint_every = 100 #@param {type:\"integer\"}\n",
        "demo_every = 25 #@param {type:\"integer\"}\n",
        "num_demos = 1 #@param {type:\"integer\"}\n",
        "demo_samples = 65536 #@param {type:\"integer\"}\n",
        "demo_steps = 50 #@param {type:\"integer\"}\n",
        "accum_batches = 1 #@param {type:\"integer\"}\n",
        "training_dir = '/content/drive/MyDrive/AI/datasets/DD/dub_neuro' #@param {type:\"string\"}\n",
        "base_model_path = '/content/drive/MyDrive/AI/models/DanceDiffusion/dd/base_models/jmann-large-580k.ckpt' #@param {type:\"string\"}\n",
        "\n",
        "sample_size = 65536 #@param {type:\"integer\"}\n",
        "sample_rate = 48000 #@param {type:\"integer\"}\n",
        "latent_dim = 0 #@param {type:\"integer\"}\n",
        "seed = 0 #@param {type:\"integer\"}\n",
        "batch_size = 4 #@param {type:\"integer\"}\n",
        "max_epochs = 1000000 #@param {type:\"integer\"}\n",
        "lora_rank = 16 #@param {type:\"integer\"}\n",
        "\n",
        "ema_decay = 0.95 #@param {type:\"number\"}\n",
        "random_crop = False #@param {type:\"boolean\"}\n",
        "num_gpus = 1 #@param {type:\"integer\"}\n",
        "cache_training_data = False #@{type:\"boolean\"}\n",
        "\n",
        "args = Object()\n",
        "\n",
        "args.run_name = run_name\n",
        "args.project_name = project_name\n",
        "args.checkpoint_every = checkpoint_every\n",
        "args.demo_every = demo_every\n",
        "args.num_demos = num_demos\n",
        "args.demo_samples = demo_samples\n",
        "args.demo_steps = demo_steps\n",
        "args.accum_batches = accum_batches\n",
        "args.training_dir = training_dir\n",
        "args.base_model_path = base_model_path\n",
        "\n",
        "args.sample_size = sample_size\n",
        "args.sample_rate = sample_rate\n",
        "args.latent_dim = latent_dim\n",
        "args.seed = seed\n",
        "args.batch_size = batch_size\n",
        "args.max_epochs = max_epochs\n",
        "args.lora_rank = lora_rank\n",
        "args.ema_decay = ema_decay\n",
        "args.random_crop = random_crop\n",
        "args.num_gpus = num_gpus\n",
        "args.cache_training_data = cache_training_data\n",
        "\n",
        "device_type_accelerator = get_torch_device_type()\n",
        "device_accelerator = torch.device(device_type_accelerator)\n",
        "\n",
        "\n",
        "\n",
        "wandb_logger = pl.loggers.WandbLogger(project=args.project_name, name=args.run_name)\n",
        "torch.manual_seed(seed=args.seed)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aPwjlTfkWh72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Model & Inject\n",
        "model = DiffusionUncondLora.load_from_checkpoint(args.base_model_path, map_location=device_accelerator, global_args=args, strict=False)\n",
        "model.inject_new_lora(lora_dim=args.lora_rank)\n",
        "model.to(device_accelerator)\n"
      ],
      "metadata": {
        "id": "Up_ME6wlZVcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start Training\n",
        "\n",
        "# Load dataset\n",
        "train_set = SampleDataset([args.training_dir], args)\n",
        "train_dl = data.DataLoader(\n",
        "    train_set,\n",
        "    args.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "\n",
        "from typing import Any\n",
        "\n",
        "class HijackedModelCheckpoint(pl.callbacks.ModelCheckpoint):\n",
        "    def _save_checkpoint(self, trainer: \"pl.Trainer\", filepath: str) -> None:\n",
        "        trainer.model.lora.save_weights(filepath, device_accelerator)\n",
        "\n",
        "        self._last_global_step_saved = trainer.global_step\n",
        "\n",
        "        # notify loggers\n",
        "        if trainer.is_global_zero:\n",
        "            for logger in trainer.loggers:\n",
        "                logger.after_save_checkpoint(proxy(self))\n",
        "\n",
        "exc_callback = ExceptionCallback()\n",
        "ckpt_callback = HijackedModelCheckpoint(every_n_train_steps=args.checkpoint_every, save_top_k=-1, dirpath='output')\n",
        "demo_callback = DemoCallback(args)\n",
        "\n",
        "wandb_logger.watch(model)\n",
        "wandb_logger.config = args\n",
        "\n",
        "diffusion_trainer = pl.Trainer(\n",
        "    devices=args.num_gpus,\n",
        "    accelerator=\"gpu\",\n",
        "    num_nodes = args.num_nodes,\n",
        "    strategy='ddp',\n",
        "    precision=16,\n",
        "    accumulate_grad_batches=args.accum_batches,\n",
        "    callbacks=[ckpt_callback, demo_callback, exc_callback],\n",
        "    logger=wandb_logger,\n",
        "    log_every_n_steps=1,\n",
        "    max_epochs=args.max_epochs,\n",
        "\n",
        ") if args.num_gpus > 1 else pl.Trainer(\n",
        "    devices=1,\n",
        "    accelerator=\"gpu\",\n",
        "    precision=16,\n",
        "    accumulate_grad_batches=args.accum_batches,\n",
        "    callbacks=[ckpt_callback, demo_callback, exc_callback],\n",
        "    logger=wandb_logger,\n",
        "    log_every_n_steps=1,\n",
        "    max_epochs=args.max_epochs,\n",
        ")\n",
        "\n",
        "diffusion_trainer.fit(model, train_dl)\n",
        "import wandb\n",
        "wandb.finish()\n",
        "\n"
      ],
      "metadata": {
        "id": "c8WFUONKYCDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}